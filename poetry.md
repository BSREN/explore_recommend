深度学习是一类机器学习方法，可实例化为深度学习器，所对应的设计、训练和使用方法集合称为深度学习。

深度学习器由若干处理层组成，每层包含至少一个处理单元，每层输出为数据的一种表征，且表征层次随处理层次增加而提高。

深度的定义是相对的。针对某具体场景和学习任务，若学习器的处理单元总数和层数分别为M和N，学习器所保留的信息量或任务性能超过任意层数小于N且单元总数为M的学习器，则该学习器为严格的或狭义的深度学习器，其对应的设计、训练和使用方法集合为严格的或狭义的深度学习。

广义的深度学习器及对应的深度学习方法可依据经验和局部最优化设计，不进行上述严格的遍历比较。

深度学习听起来高深，落地的应用却可以很浪漫。比如作诗、作曲、人脸美容美妆等都可以利用深度学习算法来实现。这次我们主要介绍深度学习在古诗词生成器方面的应用。

下面我们以古诗词生成器为例，一步一步带你从数据处理到模型搭建，再到训练出古诗词生成模型

### LSTM介绍
像诗词文本这样的数据，文字的前后文存在关联性被称为序列化数据，即前一数据和后一个数据有顺序关系。深度学习中有一个重要的分支是专门用来处理这样的数据的——循环神经网络。循环神经网络广泛应用在自然语言处理领域(NLP)，今天我们带你从一个实际的例子出发，介绍循环神经网络一个重要的改进算法模型-LSTM。本文章不对LSTM的原理进行深入，想详细了解LSTM的可以参考下面的[文章链接](https://www.jianshu.com/p/9dc9f41f0b29)。

下面我们以古诗词生成器为例，一步一步带你从数据处理到模型搭建，再到训练出古诗词生成模型，最后实现用古诗词自动生成祝福诗词。

### 数据处理

我们使用76748首古诗词作为数据集，数据集[下载链接](http://www.momodel.cn:8899/#/explore/5c00a6e21afd942b66b36ba8?type=dataset)，原始的古诗词的存储形式如下：
![image](https://user-images.githubusercontent.com/43362551/51824023-221ea180-231c-11e9-8577-6595844d752f.png)
我们可以看到原始的古诗词是文本符号的形式，无法直接进行机器学习，所以我们第一步需要把文本信息转换为数据形式，这种转换方式就叫词嵌入(word embedding)，我们采用一种常用的词嵌套(word embedding)算法-Word2vec对古诗词进行编码。关于Word2Vec这里不详细讲解，有兴趣的可以参考下面的[文章链接](https://zhuanlan.zhihu.com/p/26306795)。在词嵌套过程中，为了避免最终的分类数过于庞大，可以选择去掉出现频率较小的字，比如可以去掉只出现过一次的字。Word2vec算法经过训练后会产生一个模型文件，我们就可以利用这个模型文件对古诗词文本进行词嵌套编码。

经过第一步的处理已经把古诗词词语转换为可以机器学习建模的数字形式，因为我们采用LSTM算法进行古诗词生成，所以还需要构建输入到输出的映射处理。例如：
“[长河落日圆]”作为train_data，而相应的train_label就是“长河落日圆]]”，也就是
“[”->“长”，“长”->“河”，“河”->“落”，“落”->“日”，“日”->“圆”，“圆”->“]”，“]”->“]”，这样子先后顺序一一对相。这也是循环神经网络的一个重要的特征。
这里的“[”和“]”是开始符和结束符，用于生成古诗的开始与结束标记。

总结一下数据处理的步骤：
- 读取原始的古诗词文本，统计出所有不同的字，使用 Word2Vec 算法进行对应编码；
- 对于每首诗，将每个字、标点都转换为字典中对应的编号，构成神经网络的输入数据 train_data；
- 将输入数据左移动构成输出标签 train_label；

经过数据处理后我们得到以下数据文件： 
- poems_edge_split.txt：原始古诗词文件，按行排列，每行为一首诗词；
- vectors_poem.bin：利用 Word2Vec训练好的词向量模型，以</s>开头，按词频排列，去除低频词；
- poem_ids.txt：按输入输出关系映射处理之后的语料库文件；
- rhyme_words.txt： 押韵词存储，用于押韵诗的生成；

在提供的源码中已经提供了以上四个数据文件放在data文件夹下，数据处理代码见 data_loader.py 文件，[源码链接](http://www.momodel.cn:8899/#/explore/5bfd118f1afd942b66b36b30?type=app)


### 模型构建及训练
这里我们使用2层的LSTM框架，每层有128个隐藏层节点，我们使用tensorflow.nn模块库来定义网络结构层，其中RNNcell是tensorflow中实现RNN的基本单元，是一个抽象类，在实际应用中多用RNNcell的实现子类BasicRNNCell或者BasicLSTMCell，BasicGRUCell；如果需要构建多层的RNN，在TensorFlow中，可以使用tf.nn.rnn_cell.MultiRNNCell函数对RNNCell进行堆叠。模型网络的第一层要对输入数据进行 embedding，可以理解为数据的维度变换，经过两层LSTM后，接着softMax得到一个在全字典上的输出概率。
模型网络结构如下：
![image](https://user-images.githubusercontent.com/43362551/51891576-8142eb80-23da-11e9-84c4-66ffdf971818.png)

训练时可以定义batch_size的值，是否进行dropout，为了结果的多样性，训练时在softmax输出层每次可以选择topK概率的字符作为输出。训练完成后可以使用tensorboard 对网络结构和训练过程可视化展示。这里推荐大家一个在线人工智能建模平台[momodel.cn](http://www.momodel.cn:8899/#/)，带有完整的Python和机器学习框架运行环境，并且有免费的GPU可以使用，大家可以训练的时候可以在这个平台上试一下。训练部分的代码和训练好的模型见。

### 诗词生成
调用前面训练好的模型我们就可以实现一个古诗词的应用了，我这里是在[人工智能建模平台 Mo](http://www.momodel.cn:8899/#/)中实现的，实现了藏头诗和藏子诗自动生成的功能，运行的效果如下：
![image](https://user-images.githubusercontent.com/43362551/51896208-e69cd980-23e6-11e9-9f0a-c6d042b8f5af.png)
![image](https://user-images.githubusercontent.com/43362551/51897625-23b69b00-23ea-11e9-8640-0b64c8252562.png)

新年快到了，赶紧利用算法自动作诗，给亲戚朋友送去最好的祝福

